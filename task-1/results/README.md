# Обоснование выбора

В качестве решения предлагается `Apache Airflow`. Он выступит в качестве оркестратора и позволит выстраивать гибкие ETL процессы для формирования требуемых аналитических данных.

Для Apache Airflow существуют готовые модули, позволяющие интегрироваться с требуемыми системами:

- Для `BigQuery` есть операторы в рамках пакета `apache-airflow-providers-google`;
- Для `RedShift` существует операторы в `apache-airflow-providers-amazon`;
- Для `Kafka` есть `apache-airflow-providers-apache-kafka`;
- Для `Spark` также представлено готовое решение в виде `apache-airflow-providers-apache-spark`.

Apache Airflow также поддерживает множество возможностей для ветвления процесса, ожидания триггеров для продолжения обработки и гибких вариантов запуска при помощи `BranchOperators`, `Sensors`, `Dynamic DAGs`, `Trigger Rules` и т.д.

Для каждого оператора можно настроить правила повторной обработки, а отправлять уведомления через callback с использованием нужной системы, в том числе Email с помощью `EmailOperator`.

Apache Airflow может быть развернут в Kubernetes (существует официальный Helm-чарт), где в качестве зависимостей (помимо оркестрируемых систем и процессов) он потребует базу метаданных (например, PostgreSQL). Кроме того, Airflow позволяет запускать отдельные воркеры в виде подов (при помощи KubernetesExecutor/CeleryKubernetesExecutor), что помогает гибко масштабировать обработку.

## Proof of Concept

Небольшой DAG, который читает данные о заказах из postgres базы данных и csv файла, считает количество ошибок заказов, и если их больше 10%, отправляет уведомление на почту, а если меньше, то ничего не делает.

![success](./dag_success.png?raw=true "Успешное выполнение")

 При этом все таски поддерживают механизм повторной обработки, а при окончательной ошибке отправляется сообщение.

 ![notices](./dag_success.png?raw=true "Увеодмления на почте")

 Для запуска выполните:

 ```sh
 docker compose -f ./compose.yaml -f ./compose.airflow.yaml up --force-recreate -d
```

Кроме того нужно настроить подключение с id `test_db` к postgres базе данных, строка подключения: `Host=data_source_db;Database=test_db;Port=5432;Username=postgres;Password=postgres`.
