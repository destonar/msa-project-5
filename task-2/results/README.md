# Выбор решения для пакетной обработки данных

## Требования

Каждое утро ровно в шесть онлайн-магазину нужно сгенерировать кастомные CSV/XLS прайс-листы для B2B-клиентов на основе текущих данных в БД. Процесс выгрузки не требует дополнительной логики по обработке данных.

Данные в PostgreSQL представлены в таблицах:

- products ~ 5 000–10 000 строк;
- categories ~ 50–200 строк;
- clients ~ 100–500 строк;
- client_prices ~ 10 000–20 000 строк.

Необходимо связать данные из таблиц и выгрузить их в CSV-файл. Объём данных 5 000 –10 000 строк. Инфраструктура магазина — микросервисы в облаке.

## Описание решения

В качестве решения в данном случае предлагается использовать `Kubernetes CronJob`, который будет запускать простое приложение-экспортер. Так как объемы данных весьма низкие, а дополнительной логики при обработке не требуется, в качестве приоритетного варианта следует рассматривать именно его. Однако, если в будущем сложность логики преобразования и/или формирования конечных данных будет увеличиваться, можно рассмотреть `Apache Airflow`.

В текущем виде предлагается следующее решение:

```plantuml
@startuml C4_Container
!include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml

ContainerDb(db, "БД прайс-листов", "PostgreSQL")
ContainerDb(storage, "Объектное хранилище", "S3")

Container(cron_job, "Экспортер", "CronJob")

System_Ext(observability_stack, "Система мониторинга и логирования", "Prometheus, Grafana + ELK")

Rel(db, cron_job, "Получает данные", "SQL")
Rel(cron_job, storage, "Сохраняет результат", "HTTP")
Rel(cron_job, observability_stack, "Передача метрик и логов")

@enduml
```

Здесь предполагается, что в компании уже существует система мониторинга и логирования, на которую заодно можно будет положиться для уведомлений об ошибках.

## Сравнение с альтернативами

| | SpringBatch | ApacheAirflow | K8s Job | Spark |
| --- | --- | --- | --- | --- |
| Наличие конфигурации CRON-расписания | Встроенного механизма нет, но можно использовать Spring Scheduler или Quartz | Возможность задавать расписание запусков через schedule_interval и UI | Да, через CronJob | Не имеет функционала расписания |
| Сложность реализации логики по обработке данных | Низкая, готовые  инструменты как Job,Step, ItemWriter | Низкая, готовые операторы, а также множество готовых решений по взаимодействию с популярными системами | Средняя, зависит от сложности задач и выбранного фреймворка, так как вся обработка должна быть реализована самостоятельно | Высокая, рассчитана на большие потоки данных |
| Ресурсоемкость решения (количество потребляемых ресурсов) | Низкая, требует постоянно запущенного приложения | Средняя, так как требует запуска БД, планировщика, воркеров, вебсервера и тп | Низкая, при условии уже развернутого Kubernetes, | Высокая, требуется тяжеловесный Spark-кластер |
| Масштабируемость решения под нагрузкой и сложность реализации | Ограниченная: масштабирование шагов, партиции | Хорошая, executor'ы легко масштабируются | Легко масштабируются на уровне подов, однако нужно самостоятельно следить за соблюдением целостности данных | Отличная для больших данных |
| Сложность развертывания в облаке и интеграция с имеющейся микросервисной  архитектурой | Низкая, легко контейнеризировать | Средняя, есть официальный Helm-чарт | Минимальная | Высокая, требуется Spark-кластер |
| Удобство интеграции с системами логирования и мониторинга | Легко подключить с использованием, например, Spring Actuator | Встроенные логи Airflow, есть интеграция с Prometheus и Grafana | Зависит от кластера и внешних инструментов, но в большинстве случаев сложность невысокая | Через Spark UI, требует отдельной инфраструктуры |

## Базовая конфигурация

Сам экспортер должен быть крайне простым: он должен в цикле выполнять запросы, которые ограничивают размер пакета при помощи offset и limit.

Запись csv должна происходить на диск для минимизации потребления памяти, после чего загружаться в S3 хранилище после окончания работы.

При ошибке рестарт всегда будет сначала, так как это проще и на таком объеме данных не имеет большого значения.

Важно заметить, что в самом простом виде экспортер не сможет поддерживать параллельную обработку, а потому параллелизм джобов следует отключить в явном виде.

При увеличении объема данных следует рассмотреть переход, например, к Apache Airflow.

Пример конфигурации для CronJob:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: exporter-job
spec:
  schedule: "0 6 * * *"
  timeZone: "Europe/Moscow"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 3
      template:
        spec:
          containers:
          - name: exporter-job
            image: exporter-job
            env:
              - name: POSTGRES_CONNECTION
                valueFrom:
                  secretKeyRef:
                    name: postgres-credentials
                    key: connection
              - name: STORAGE_CONNECTION
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: connection
              - name: BATCH_SIZE
                value: 500
            resources:
              requests:
                cpu: "100m"
                memory: "256Mi"
              limits:
                cpu: "500m"
                memory: "512Mi"
          restartPolicy: OnFailure
```
